{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build_GNN_2: message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from MPNN_featurize import featurize\n",
    "from build_GNN_1 import ProteinFeatures, gather_edges, PositionalEncodings\n",
    "\n",
    "batch = [\n",
    "    {\n",
    "        'seq_chain_A': 'MKLVFLVLLVFVQGF',\n",
    "        'coords_chain_A': {'N_chain_A': np.random.rand(15, 3), 'CA_chain_A': np.random.rand(15, 3), 'C_chain_A': np.random.rand(15, 3), 'O_chain_A': np.random.rand(15, 3)},\n",
    "        'seq_chain_B': 'MSVKVEEVG',\n",
    "        'coords_chain_B': {'N_chain_B': np.random.rand(9, 3), 'CA_chain_B': np.random.rand(9, 3), 'C_chain_B': np.random.rand(9, 3), 'O_chain_B': np.random.rand(9, 3)},\n",
    "        'seq_chain_C': 'ATCGATCGATCGATCG',\n",
    "        'coords_chain_C': {'N_chain_C': np.random.rand(16, 3), 'CA_chain_C': np.random.rand(16, 3), 'C_chain_C': np.random.rand(16, 3), 'O_chain_C': np.random.rand(16, 3)},\n",
    "        'masked_list': ['A', 'B'],\n",
    "        'visible_list': ['C'],\n",
    "        'num_of_chains': 3,\n",
    "        'seq': 'MKLVFLVLLVFVQGF'+ 'MSVKVEEVG' + 'ATCGATCGATCGATCG'\n",
    "    },\n",
    "      {\n",
    "        'seq_chain_X': 'ACDEFGHIKLMNPQRSTVWY',\n",
    "        'coords_chain_X': {'N_chain_X': np.random.rand(20, 3), 'CA_chain_X': np.random.rand(20, 3), 'C_chain_X': np.random.rand(20, 3), 'O_chain_X': np.random.rand(20, 3)},\n",
    "        'seq_chain_Y': 'ACCDEFGHILKLM',\n",
    "        'coords_chain_Y': {'N_chain_Y': np.random.rand(13, 3), 'CA_chain_Y': np.random.rand(13, 3), 'C_chain_Y': np.random.rand(13, 3), 'O_chain_Y': np.random.rand(13, 3)},\n",
    "        'seq_chain_Z': 'LKLMNRPQRST',\n",
    "        'coords_chain_Z': {'N_chain_Z': np.random.rand(11, 3), 'CA_chain_Z': np.random.rand(11, 3), 'C_chain_Z': np.random.rand(11, 3), 'O_chain_Z': np.random.rand(11, 3)},\n",
    "        'masked_list': ['X', 'Y'],\n",
    "        'visible_list': ['Z'],\n",
    "        'num_of_chains': 3,\n",
    "        'seq': 'ACDEFGHIKLMNPQRSTVWY'+'ACCDEFGHILKLM'+'LKLMNRPQRST'\n",
    "\n",
    "    }\n",
    "]\n",
    "device='cuda'\n",
    "X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
    "demo = ProteinFeatures(\n",
    "    edge_features=16, node_features=16, num_positional_embeddings=16,\n",
    "    num_rbf=16, top_k=30, augment_eps=0., num_chain_embeddings=16\n",
    ").to(device)\n",
    "E, E_idx=demo.forward(X,chain_M, residue_idx, chain_encoding_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize h_V with zero match h_E dim\n",
    "import torch\n",
    "h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=E.device)\n",
    "# MLP of h_E, we omit it here\n",
    "edge_features=E.shape[-1],\n",
    "hidden_dim=16\n",
    "W_e = torch.nn.Linear(edge_features, hidden_dim, bias=True).to(device)\n",
    "h_E = W_e(E)\n",
    "print(h_E.shape)\n",
    "\n",
    "# mask  [B, L, L]\n",
    "def gather_nodes(nodes, neighbor_idx):\n",
    "    # Features [B,L,C] at Neighbor indices [B,L,K] => [B,N,K,C]\n",
    "    # Flatten and expand indices per batch [B,L,K] => [B,LK] => [B,NK,C]\n",
    "    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1)) # [B, LK]\n",
    "    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2)) #[B,Lk,1]->[b,lk,l]\n",
    "    # Gather and re-pack\n",
    "    # nodes [B,L,L,1]\n",
    "    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n",
    "    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n",
    "    return neighbor_features\n",
    "\n",
    "# Encoder is unmasked self-attention\n",
    "mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1) # [B,L,K]\n",
    "mask_attend = mask.unsqueeze(-1) * mask_attend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN ENCODE LAYER\n",
    "update h_E, h_v using GNN\n",
    "Here is how they are updated:\n",
    "message = Mean_Aggreg(3layerMLP(concat(edge feature, neighbours node features))\n",
    "h_V =  Norm(h_V+mesage))\n",
    "h_v = Norm( h_V+ MLP(h_V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Code\n",
    "import torch.nn as nn\n",
    "def gather_nodes(nodes, neighbor_idx):\n",
    "    # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]\n",
    "    # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]\n",
    "    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1))\n",
    "    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
    "    # Gather and re-pack\n",
    "    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n",
    "    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n",
    "    return neighbor_features  # [B, N, K, C]\n",
    "\n",
    "def gather_nodes_t(nodes, neighbor_idx):\n",
    "    # Features [B,N,C] at Neighbor index [B,K] => Neighbor features[B,K,C]\n",
    "    idx_flat = neighbor_idx.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
    "    neighbor_features = torch.gather(nodes, 1, idx_flat)\n",
    "    return neighbor_features\n",
    "\n",
    "def cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n",
    "    h_nodes = gather_nodes(h_nodes, E_idx)  # [B,L,K,C] \n",
    "    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n",
    "    return h_nn\n",
    "\n",
    "# not sure the meaning of positionWise but it is a simple 2 layer MLP\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, num_hidden, num_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.W_in = nn.Linear(num_hidden, num_ff, bias=True)\n",
    "        self.W_out = nn.Linear(num_ff, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "    def forward(self, h_V):\n",
    "        h = self.act(self.W_in(h_V))\n",
    "        h = self.W_out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class EncLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(EncLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "        self.norm3 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W11 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W12 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W13 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "\n",
    "    def forward(self, h_V, h_E, E_idx, mask_V=None, mask_attend=None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx) # get neighbers' node feature [B,N,K,C1] and add them to edge feature [B,N,K,c2] ->[B, N, K, C1+c2]\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1) # [B,N,C] 2 [B, N, 1, C] 2 [B, N, K, C]\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1) # concat self node feature with neighours' node&edge feature  -> [B, N, K, C1+C2+C3]\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV))))) # 3 layers of MLP to generate message\n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "        dh = torch.sum(h_message, -2) / self.scale # sum over K (neighour dimension) then scale-> aggregate message\n",
    "        h_V = self.norm1(h_V + self.dropout1(dh)) # update h_V with aggragate message\n",
    "\n",
    "        dh = self.dense(h_V) # add h_V to new MLP to generate new message\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh)) # add residual and normalize to get final h_V\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)  # get neighbers' new node feature [B,N,K,C] and add them to edge feature [B,N,K,c]\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1) # [B, N, K, C1+c2+c3]\n",
    "        h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV))))) # 3 layer of MLP to generate new edge feature\n",
    "        h_E = self.norm3(h_E + self.dropout3(h_message)) # add residual  then norm to get final h_E\n",
    "        return h_V, h_E   # return updated h_V, h_E afte one round of message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch(env)",
   "language": "python",
   "name": "pytroch_kern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
