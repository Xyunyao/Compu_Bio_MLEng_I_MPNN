{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build_GNN_2: message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from MPNN_featurize import featurize\n",
    "from build_GNN_1 import ProteinFeatures, gather_edges, PositionalEncodings\n",
    "\n",
    "batch = [\n",
    "    {\n",
    "        'seq_chain_A': 'MKLVFLVLLVFVQGF',\n",
    "        'coords_chain_A': {'N_chain_A': np.random.rand(15, 3), 'CA_chain_A': np.random.rand(15, 3), 'C_chain_A': np.random.rand(15, 3), 'O_chain_A': np.random.rand(15, 3)},\n",
    "        'seq_chain_B': 'MSVKVEEVG',\n",
    "        'coords_chain_B': {'N_chain_B': np.random.rand(9, 3), 'CA_chain_B': np.random.rand(9, 3), 'C_chain_B': np.random.rand(9, 3), 'O_chain_B': np.random.rand(9, 3)},\n",
    "        'seq_chain_C': 'ATCGATCGATCGATCG',\n",
    "        'coords_chain_C': {'N_chain_C': np.random.rand(16, 3), 'CA_chain_C': np.random.rand(16, 3), 'C_chain_C': np.random.rand(16, 3), 'O_chain_C': np.random.rand(16, 3)},\n",
    "        'masked_list': ['A', 'B'],\n",
    "        'visible_list': ['C'],\n",
    "        'num_of_chains': 3,\n",
    "        'seq': 'MKLVFLVLLVFVQGF'+ 'MSVKVEEVG' + 'ATCGATCGATCGATCG'\n",
    "    },\n",
    "      {\n",
    "        'seq_chain_X': 'ACDEFGHIKLMNPQRSTVWY',\n",
    "        'coords_chain_X': {'N_chain_X': np.random.rand(20, 3), 'CA_chain_X': np.random.rand(20, 3), 'C_chain_X': np.random.rand(20, 3), 'O_chain_X': np.random.rand(20, 3)},\n",
    "        'seq_chain_Y': 'ACCDEFGHILKLM',\n",
    "        'coords_chain_Y': {'N_chain_Y': np.random.rand(13, 3), 'CA_chain_Y': np.random.rand(13, 3), 'C_chain_Y': np.random.rand(13, 3), 'O_chain_Y': np.random.rand(13, 3)},\n",
    "        'seq_chain_Z': 'LKLMNRPQRST',\n",
    "        'coords_chain_Z': {'N_chain_Z': np.random.rand(11, 3), 'CA_chain_Z': np.random.rand(11, 3), 'C_chain_Z': np.random.rand(11, 3), 'O_chain_Z': np.random.rand(11, 3)},\n",
    "        'masked_list': ['X', 'Y'],\n",
    "        'visible_list': ['Z'],\n",
    "        'num_of_chains': 3,\n",
    "        'seq': 'ACDEFGHIKLMNPQRSTVWY'+'ACCDEFGHILKLM'+'LKLMNRPQRST'\n",
    "\n",
    "    }\n",
    "]\n",
    "device='cuda'\n",
    "X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
    "demo = ProteinFeatures(\n",
    "    edge_features=16, node_features=16, num_positional_embeddings=16,\n",
    "    num_rbf=16, top_k=30, augment_eps=0., num_chain_embeddings=16\n",
    ").to(device)\n",
    "E, E_idx=demo.forward(X,chain_M, residue_idx, chain_encoding_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 44, 30, 16])\n",
      "torch.Size([2, 44, 30])\n",
      "torch.Size([2, 44, 30])\n"
     ]
    }
   ],
   "source": [
    "# initialize h_V with zero match h_E dim\n",
    "import torch\n",
    "h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=E.device) #[N,L,C]\n",
    "# MLP of h_E, we omit it here\n",
    "edge_features=E.shape[-1]\n",
    "hidden_dim=16\n",
    "W_e = torch.nn.Linear(edge_features, hidden_dim, bias=True).to(device)\n",
    "h_E = W_e(E)\n",
    "print(h_E.shape)\n",
    "\n",
    "# mask  [B, L]\n",
    "def gather_nodes(nodes, neighbor_idx):\n",
    "    # Features [B,L,C] at Neighbor indices [B,L,K] => [B,N,K,C]\n",
    "    # Flatten and expand indices per batch [B,L,K] => [B,LK] => [B,NK,C]\n",
    "    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1)) # [B, LK]\n",
    "    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2)) #[B,Lk,1]->[b,lk,c]\n",
    "    # Gather and re-pack\n",
    "    # nodes [B,L,L,1]\n",
    "    neighbor_features = torch.gather(nodes, 1, neighbors_flat) # [b, lk, C]\n",
    "    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1]) # B, L, K, C\n",
    "    return neighbor_features\n",
    "\n",
    "# Encoder is unmasked self-attention\n",
    "mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1) # [B,L,K]\n",
    "print(mask_attend.shape)\n",
    "mask_attend = mask.unsqueeze(-1) * mask_attend # [B, L, 1]*[B, L, K] # make sure only true residue has neighours padding mask\n",
    "print(mask_attend.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN ENCODE LAYER\n",
    "update h_E, h_v using GNN\n",
    "Here is how they are updated:\n",
    "message = Mean_Aggreg(3layerMLP(concat(edge feature, neighbours node features))\n",
    "h_V =  Norm(h_V+mesage))\n",
    "h_v = Norm( h_V+ MLP(h_V))\n",
    "\n",
    "Then edge feature h_E is updated based on updated h_V\n",
    "message = 3layerMLP(concat(edge feature, neighbours node features)\n",
    "h_E = Norm(h_E + message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Code\n",
    "import torch.nn as nn\n",
    "def gather_nodes(nodes, neighbor_idx):\n",
    "    # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]\n",
    "    # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]\n",
    "    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1))\n",
    "    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
    "    # Gather and re-pack\n",
    "    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n",
    "    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n",
    "    return neighbor_features  # [B, N, K, C]\n",
    "\n",
    "def gather_nodes_t(nodes, neighbor_idx):\n",
    "    # Features [B,N,C] at Neighbor index [B,K] => Neighbor features[B,K,C]\n",
    "    idx_flat = neighbor_idx.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
    "    neighbor_features = torch.gather(nodes, 1, idx_flat)\n",
    "    return neighbor_features\n",
    "\n",
    "def cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n",
    "    h_nodes = gather_nodes(h_nodes, E_idx)  # [B,L,K,C] \n",
    "    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n",
    "    return h_nn\n",
    "\n",
    "# not sure the meaning of positionWise but it is a simple 2 layer MLP\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, num_hidden, num_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.W_in = nn.Linear(num_hidden, num_ff, bias=True)\n",
    "        self.W_out = nn.Linear(num_ff, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "    def forward(self, h_V):\n",
    "        h = self.act(self.W_in(h_V))\n",
    "        h = self.W_out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class EncLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(EncLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "        self.norm3 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W11 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W12 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W13 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "\n",
    "    def forward(self, h_V, h_E, E_idx, mask_V=None, mask_attend=None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx) # get neighbers' node feature [B,N,K,C1] and add them to edge feature [B,N,K,c2] ->[B, N, K, C1+c2]\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1) # [B,N,C] 2 [B, N, 1, C] 2 [B, N, K, C]\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1) # concat self node feature with neighours' node&edge feature  -> [B, N, K, C1+C2+C3]\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV))))) # 3 layers of MLP to generate message\n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "        dh = torch.sum(h_message, -2) / self.scale # sum over K (neighour dimension) then scale-> aggregate message\n",
    "        h_V = self.norm1(h_V + self.dropout1(dh)) # update h_V with aggragate message\n",
    "\n",
    "        dh = self.dense(h_V) # add h_V to new MLP to generate new message\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh)) # add residual and normalize to get final h_V\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "\n",
    "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)  # get neighbers' new node feature [B,N,K,C] and add them to edge feature [B,N,K,c]\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_EV], -1) # [B, N, K, C1+c2+c3]\n",
    "        h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV))))) # 3 layer of MLP to generate new edge feature\n",
    "        h_E = self.norm3(h_E + self.dropout3(h_message)) # add residual  then norm to get final h_E\n",
    "        return h_V, h_E   # return updated h_V, h_E afte one round of message passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 44, 16])\n",
      "torch.Size([2, 44, 30, 16])\n"
     ]
    }
   ],
   "source": [
    "#h_EV C1+C2+c3=16*3\n",
    "device='cuda'\n",
    "num_hidden, num_in=16, 32\n",
    "test = EncLayer(num_hidden=num_hidden, num_in=num_in).to(device)\n",
    "h_V, h_E=test.forward (h_V, h_E, E_idx, mask, mask_attend)\n",
    "print(h_V.shape)\n",
    "print(h_E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode layer\n",
    "updated h_V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 44])\n"
     ]
    }
   ],
   "source": [
    "print(S.shape) # N,L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 44, 16])\n",
      "torch.Size([2, 44, 30, 32])\n",
      "torch.Size([2, 44, 30, 48])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate sequence embeddings for autoregressive decoder\n",
    "sequence_features=21  # One-hot for AA\n",
    "W_s = torch.nn.Embedding(sequence_features, hidden_dim).to(device)\n",
    "h_S = W_s(S)   # N, L, hidden\n",
    "print(h_S.shape) # N, L, hidden\n",
    "h_ES = cat_neighbors_nodes(h_S, h_E, E_idx)  #gather neighours' seq feature add to h_E\n",
    "print(h_ES.shape)\n",
    "# Build encoder embeddings  initialize X with 0\n",
    "h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
    "h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
    "print(h_EXV_encoder.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure only masked residue and true residue(not padding) has value 1 (maksed)\n",
    "chain_M = chain_M*mask #update chain_M to include missing regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "abs(): argument 'input' (position 1) must be Tensor, not builtin_function_or_method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randn\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#if not use_input_decoding_order:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m decoding_order \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort((chain_M\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\u001b[38;5;241m*\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m)\u001b[49m)) \u001b[38;5;66;03m#[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m mask_size \u001b[38;5;241m=\u001b[39m E_idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# [N, L, K]->l\u001b[39;00m\n\u001b[1;32m      5\u001b[0m permutation_matrix_reverse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(decoding_order, num_classes\u001b[38;5;241m=\u001b[39mmask_size)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;66;03m# index --> onehot [B,L,L]\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: abs(): argument 'input' (position 1) must be Tensor, not builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "from torch import randn\n",
    "#if not use_input_decoding_order:\n",
    "decoding_order = torch.argsort((chain_M+0.0001)*(torch.abs(randn))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
    "mask_size = E_idx.shape[1] # [N, L, K]->l\n",
    "permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float() # index --> onehot [B,L,L]\n",
    "# using permuatation_matrix to generate the right decoding masks\n",
    "# for example: 1th place in decoding with [ 0, 1, 0, 0] means the 2nd residue will be the first one to decode, but it is reversely coded as [0,1, 0,0] means except 2nd residue all the others are known\n",
    "order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
    "# obtain neighbours decoding mask [B,L,K,L]\n",
    "mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
    "\n",
    "mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1]) # [B, L, 1, 1]\n",
    "mask_bw = mask_1D * mask_attend  # [B, L, K, L] make padding to be zero\n",
    "mask_fw = mask_1D * (1. - mask_attend) # forward decodeing mask\n",
    "\n",
    "h_EXV_encoder_fw = mask_fw * h_EXV_encoder  #[B,L,K,L] * [B,L,K,F]\n",
    "print(h_EXV_encoder_fw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder layer\n",
    "\n",
    "class DecLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
    "        super(DecLayer, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_in = num_in\n",
    "        self.scale = scale\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_hidden)\n",
    "        self.norm2 = nn.LayerNorm(num_hidden)\n",
    "\n",
    "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
    "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
    "\n",
    "    def forward(self, h_V, h_E, mask_V=None, mask_attend=None):\n",
    "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
    "\n",
    "        # Concatenate h_V_i to h_E_ij\n",
    "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_E.size(-2),-1)\n",
    "        h_EV = torch.cat([h_V_expand, h_E], -1)\n",
    "\n",
    "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
    "        if mask_attend is not None:\n",
    "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
    "        dh = torch.sum(h_message, -2) / self.scale\n",
    "\n",
    "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
    "\n",
    "        # Position-wise feedforward\n",
    "        dh = self.dense(h_V)\n",
    "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
    "\n",
    "        if mask_V is not None:\n",
    "            mask_V = mask_V.unsqueeze(-1)\n",
    "            h_V = mask_V * h_V\n",
    "        return h_V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Triangular Mask:\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "\n",
      "Permutation Matrix:\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "\n",
      "Randomized Autoregressive Mask:\n",
      "[[1. 1. 1. 0. 1.]\n",
      " [0. 1. 1. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Create the lower triangular matrix\n",
    "def lower_triangular_mask(length):\n",
    "    return np.tril(np.ones((length, length)))\n",
    "\n",
    "# Step 2: Generate a permutation matrix for the given sequence\n",
    "def permutation_matrix(order):\n",
    "    n = len(order)\n",
    "    perm_matrix = np.zeros((n, n))\n",
    "    for i, val in enumerate(order):\n",
    "        perm_matrix[i, val - 1] = 1\n",
    "    return perm_matrix\n",
    "\n",
    "# Step 3: Generate the randomized autoregressive mask\n",
    "def generate_mask(order):\n",
    "    n = len(order)\n",
    "    seq_mask = lower_triangular_mask(n)\n",
    "    perm_matrix = permutation_matrix(order)\n",
    "    return perm_matrix.T @ seq_mask @ perm_matrix\n",
    "\n",
    "# Example\n",
    "order = [3, 5, 2, 1, 4]\n",
    "mask = generate_mask(order)\n",
    "\n",
    "print(\"Lower Triangular Mask:\")\n",
    "print(lower_triangular_mask(len(order)))\n",
    "\n",
    "print(\"\\nPermutation Matrix:\")\n",
    "print(permutation_matrix(order))\n",
    "\n",
    "print(\"\\nRandomized Autoregressive Mask:\")\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmr_assign",
   "language": "python",
   "name": "nmr_assign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
