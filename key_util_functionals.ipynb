{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key untils functions breakdown\n",
    "\n",
    "MPNN is based on GNN model <br>\n",
    "We need to define nodes, neighbours and edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerate training  validation and testing dataset\n",
    "train and test ID are saved into two text files<br>\n",
    "Based on cut_off(date, resolution), use list.csv as lookup table to extract <br>\n",
    "chainID and Hash ID used a format as train['cluster_Id']=[CHAINID, HASH]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from dateutil import parser\n",
    "data_path=\"/home/yunyao/MPNN_breakdown/pdb_2021aug02_sample\"\n",
    "params = {\n",
    "    \"LIST\"    : f\"{data_path}/list.csv\", \n",
    "    \"VAL\"     : f\"{data_path}/valid_clusters.txt\",\n",
    "    \"TEST\"    : f\"{data_path}/test_clusters.txt\",\n",
    "    \"DIR\"     : f\"{data_path}\",\n",
    "    \"DATCUT\"  : \"2030-Jan-01\",\n",
    "    \"RESCUT\"  : 4,      #args.rescut, #resolution cutoff for PDBs\n",
    "    \"HOMO\"    : 0.70 #min seq.id. to detect homo chains\n",
    "}\n",
    "\n",
    "def build_training_clusters(params, debug):\n",
    "    val_ids = set([int(l) for l in open(params['VAL']).readlines()])\n",
    "    test_ids = set([int(l) for l in open(params['TEST']).readlines()])\n",
    "   \n",
    "    if debug:\n",
    "        val_ids = []\n",
    "        test_ids = []\n",
    " \n",
    "    # read & clean list.csv\n",
    "    with open(params['LIST'], 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) #skip header\n",
    "        #CHAINID,DEPOSITION,RESOLUTION,HASH,CLUSTER,SEQUENCE\n",
    "        rows = [[r[0],r[3],int(r[4])] for r in reader\n",
    "                if float(r[2])<=params['RESCUT'] and\n",
    "                parser.parse(r[1])<=parser.parse(params['DATCUT'])]\n",
    "    \n",
    "    # compile training and validation sets\n",
    "    train = {}\n",
    "    valid = {}\n",
    "    test = {}\n",
    "\n",
    "    # example \n",
    "    #train['cluster_Id']=[CHAINID, HASH]\n",
    "\n",
    "    if debug:\n",
    "        rows = rows[:20]\n",
    "    for r in rows:  # r[2] is the cluster number \n",
    "        if r[2] in val_ids:  \n",
    "            if r[2] in valid.keys():\n",
    "                valid[r[2]].append(r[:2])\n",
    "            else:\n",
    "                valid[r[2]] = [r[:2]]\n",
    "        elif r[2] in test_ids:\n",
    "            if r[2] in test.keys():\n",
    "                test[r[2]].append(r[:2])\n",
    "            else:\n",
    "                test[r[2]] = [r[:2]]\n",
    "        else:\n",
    "            if r[2] in train.keys():\n",
    "                train[r[2]].append(r[:2])\n",
    "            else:\n",
    "                train[r[2]] = [r[:2]]\n",
    "    if debug:\n",
    "        valid=train       \n",
    "    return train, valid, test\n",
    "\n",
    "train, valid, test=build_training_clusters(params,debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24311\n",
      "[['2fqm_A', '028171'], ['2fqm_B', '028171'], ['2fqm_C', '028171'], ['2fqm_D', '028171'], ['2fqm_E', '028171'], ['2fqm_F', '028171']]\n"
     ]
    }
   ],
   "source": [
    "print(len(train.keys()))\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREAT pytorch dataset class \n",
    "intialize with ID (train.keys) <br>\n",
    "loader: loader function, input:  PBDID_chainID '2fqm_C' ,param['Directory']:<br>\n",
    "generate path pointing to local *.pt file <br>\n",
    "output:  return {'seq'    : seq, <br>\n",
    "            'xyz'    : torch.cat(xyz,dim=0),  \n",
    "            'idx'    : torch.cat(idx,dim=0),  \n",
    "            'masked' : torch.Tensor(masked).int(),  \n",
    "            'label'  : item[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "class PDB_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, IDs, loader, train_dict, params): \n",
    "    # loader is a loader function\n",
    "        self.IDs = IDs\n",
    "        self.train_dict = train_dict\n",
    "        self.loader = loader\n",
    "        self.params = params\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ID = self.IDs[index]\n",
    "        sel_idx = np.random.randint(0, len(self.train_dict[ID])) # select random chain within the ID\n",
    "        out = self.loader(self.train_dict[ID][sel_idx], self.params)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def loader_pdb(item,params):\n",
    "\n",
    "    pdbid,chid = item[0].split('_')\n",
    "    PREFIX = \"%s/pdb/%s/%s\"%(params['DIR'],pdbid[1:3],pdbid)\n",
    "    \n",
    "    # load metadata\n",
    "    if not os.path.isfile(PREFIX+\".pt\"):\n",
    "        return {'seq': np.zeros(5)}\n",
    "    meta = torch.load(PREFIX+\".pt\")\n",
    "    asmb_ids = meta['asmb_ids']  \n",
    "    asmb_chains = meta['asmb_chains']\n",
    "    chids = np.array(meta['chains'])\n",
    "\n",
    "    # find candidate assemblies which contain chid chain\n",
    "    asmb_candidates = set([a for a,b in zip(asmb_ids,asmb_chains)\n",
    "                           if chid in b.split(',')])\n",
    "\n",
    "    # if the chains is missing is missing from all the assemblies\n",
    "    # then return this chain alone\n",
    "    if len(asmb_candidates)<1:\n",
    "        chain = torch.load(\"%s_%s.pt\"%(PREFIX,chid))\n",
    "        L = len(chain['seq'])\n",
    "        return {'seq'    : chain['seq'],\n",
    "                'xyz'    : chain['xyz'],\n",
    "                'idx'    : torch.zeros(L).int(),\n",
    "                'masked' : torch.Tensor([0]).int(),\n",
    "                'label'  : item[0]}\n",
    "\n",
    "    # randomly pick one assembly from candidates\n",
    "    asmb_i = random.sample(list(asmb_candidates), 1)\n",
    "\n",
    "    # indices of selected transforms\n",
    "    idx = np.where(np.array(asmb_ids)==asmb_i)[0]\n",
    "\n",
    "    # load relevant chains in the assembly\n",
    "    chains = {c:torch.load(\"%s_%s.pt\"%(PREFIX,c))\n",
    "              for i in idx for c in asmb_chains[i]\n",
    "              if c in meta['chains']}\n",
    "\n",
    "    # generate assembly\n",
    "    asmb = {}\n",
    "    for k in idx:\n",
    "\n",
    "        # pick k-th xform\n",
    "        xform = meta['asmb_xform%d'%k]\n",
    "        u = xform[:,:3,:3]\n",
    "        r = xform[:,:3,3]\n",
    "\n",
    "        # select chains which k-th xform should be applied to\n",
    "        s1 = set(meta['chains'])\n",
    "        s2 = set(asmb_chains[k].split(','))\n",
    "        chains_k = s1&s2\n",
    "\n",
    "        # transform selected chains \n",
    "        for c in chains_k:\n",
    "            try:\n",
    "                xyz = chains[c]['xyz']\n",
    "                xyz_ru = torch.einsum('bij,raj->brai', u, xyz) + r[:,None,None,:]\n",
    "                asmb.update({(c,k,i):xyz_i for i,xyz_i in enumerate(xyz_ru)})\n",
    "            except KeyError:\n",
    "                return {'seq': np.zeros(5)}\n",
    "\n",
    "    # select chains which share considerable similarity to chid\n",
    "    seqid = meta['tm'][chids==chid][0,:,1]\n",
    "    homo = set([ch_j for seqid_j,ch_j in zip(seqid,chids)\n",
    "                if seqid_j>params['HOMO']])\n",
    "    # stack all chains in the assembly together\n",
    "    seq,xyz,idx,masked = \"\",[],[],[]\n",
    "    seq_list = []\n",
    "    for counter,(k,v) in enumerate(asmb.items()):\n",
    "        seq += chains[k[0]]['seq']\n",
    "        seq_list.append(chains[k[0]]['seq'])\n",
    "        xyz.append(v)\n",
    "        idx.append(torch.full((v.shape[0],),counter))\n",
    "        if k[0] in homo:\n",
    "            masked.append(counter)\n",
    "\n",
    "    return {'seq'    : seq,\n",
    "            'xyz'    : torch.cat(xyz,dim=0),\n",
    "            'idx'    : torch.cat(idx,dim=0),\n",
    "            'masked' : torch.Tensor(masked).int(),\n",
    "            'label'  : item[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert train valid test id into dataloader\n",
    "train_set = PDB_dataset(list(train.keys()), loader_pdb, train, params)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighour and edge\n",
    "algorithm: K-nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch(env)",
   "language": "python",
   "name": "pytroch_kern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
